{
  "Learn patterns for importing and exporting data in CSV and Excel formats with batch processing and validation.": "Learn patterns for importing and exporting data in CSV and Excel formats with batch processing and validation.",
  "Import/Export Patterns": "Import/Export Patterns",
  "This guide covers patterns for handling data import and export operations, including CSV processing, Excel file handling, and batch data operations with Step Functions.": "This guide covers patterns for handling data import and export operations, including CSV processing, Excel file handling, and batch data operations with Step Functions.",
  "When to Use This Guide": "When to Use This Guide",
  "Use this guide when you need to:": "Use this guide when you need to:",
  "Import bulk data from CSV or Excel files": "Import bulk data from CSV or Excel files",
  "Export data to various formats": "Export data to various formats",
  "Process large datasets with Step Functions": "Process large datasets with Step Functions",
  "Implement file upload with S3 presigned URLs": "Implement file upload with S3 presigned URLs",
  "Transform data between external and internal formats": "Transform data between external and internal formats",
  "Import Architecture Overview": "Import Architecture Overview",
  "File Upload Pattern": "File Upload Pattern",
  "Storage Service": "Storage Service",
  "Generate presigned URLs for secure file uploads:": "Generate presigned URLs for secure file uploads:",
  "Generate upload URL for file import": "Generate upload URL for file import",
  "1 hour": "1 hour",
  "Generate download URL for file export": "Generate download URL for file export",
  "Storage Controller": "Storage Controller",
  "Get presigned URL for upload": "Get presigned URL for upload",
  "Get presigned URL for download": "Get presigned URL for download",
  "CSV Import Pattern": "CSV Import Pattern",
  "CSV Import Controller": "CSV Import Controller",
  "Import type identifier": "Import type identifier",
  "Start CSV import via Step Functions": "Start CSV import via Step Functions",
  "CSV Parser Service": "CSV Parser Service",
  "Parse CSV file from S3": "Parse CSV file from S3",
  "Validate parsed rows": "Validate parsed rows",
  "Import Event Handler": "Import Event Handler",
  "Process CSV import event": "Process CSV import event",
  "Parse CSV": "Parse CSV",
  "Validate": "Validate",
  "Filter valid rows": "Filter valid rows",
  "Process in batches": "Process in batches",
  "Excel Import Pattern": "Excel Import Pattern",
  "Excel Helper Functions": "Excel Helper Functions",
  "Get cell value handling formulas and rich text": "Get cell value handling formulas and rich text",
  "Handle formula result": "Handle formula result",
  "Handle rich text": "Handle rich text",
  "Get numeric cell value": "Get numeric cell value",
  "Get date cell value": "Get date cell value",
  "Find header row by matching column headers": "Find header row by matching column headers",
  "Excel Import Service": "Excel Import Service",
  "Load workbook from S3": "Load workbook from S3",
  "For .xls files, use different parser": "For .xls files, use different parser",
  "Process worksheet with row processor": "Process worksheet with row processor",
  "Find or use specified header row": "Find or use specified header row",
  "Skip empty rows": "Skip empty rows",
  "Import Strategy Pattern": "Import Strategy Pattern",
  "Base interface for import strategies": "Base interface for import strategies",
  "Transform raw input to command DTO": "Transform raw input to command DTO",
  "Validate transformed DTO": "Validate transformed DTO",
  "Base import strategy with common functionality": "Base import strategy with common functionality",
  "Transform raw input to command DTO (default: return as-is)": "Transform raw input to command DTO (default: return as-is)",
  "Validate transformed DTO using class-validator": "Validate transformed DTO using class-validator",
  "Uses class-validator for validation": "Uses class-validator for validation",
  "Flatten validation errors to a simple format": "Flatten validation errors to a simple format",
  "The input type, must be an object": "The input type, must be an object",
  "The output DTO type, must be an object": "The output DTO type, must be an object",
  "Concrete Import Strategy": "Concrete Import Strategy",
  "Transform import data to command DTO": "Transform import data to command DTO",
  "Validate import input": "Validate import input",
  "Export Pattern": "Export Pattern",
  "Export Service": "Export Service",
  "Export data to CSV and upload to S3": "Export data to CSV and upload to S3",
  "Build CSV content": "Build CSV content",
  "Upload to S3": "Upload to S3",
  "Export data to Excel and upload to S3": "Export data to Excel and upload to S3",
  "Set columns": "Set columns",
  "Style header row": "Style header row",
  "Add data rows": "Add data rows",
  "Generate buffer": "Generate buffer",
  "Step Function Integration": "Step Function Integration",
  "Import Orchestration": "Import Orchestration",
  "Import workflow with Step Functions": "Import workflow with Step Functions",
  "Best Practices": "Best Practices",
  "1. Batch Processing": "1. Batch Processing",
  "Always process large files in batches:": "Always process large files in batches:",
  "2. Error Handling": "2. Error Handling",
  "Collect and report errors without stopping processing:": "Collect and report errors without stopping processing:",
  "Continue processing": "Continue processing",
  "3. Validation Before Processing": "3. Validation Before Processing",
  "Validate all data before starting import:": "Validate all data before starting import:",
  "First pass: validate": "First pass: validate",
  "Second pass: process": "Second pass: process",
  "4. Progress Reporting": "4. Progress Reporting",
  "Report progress for long-running imports:": "Report progress for long-running imports:",
  "Report progress every 100 rows": "Report progress every 100 rows",
  "ImportModule API Reference": "ImportModule API Reference",
  "The `@mbc-cqrs-serverless/import` package provides a comprehensive framework for managing data import tasks.": "The `@mbc-cqrs-serverless/import` package provides a comprehensive framework for managing data import tasks.",
  "Installation": "Installation",
  "ProcessingMode Enum": "ProcessingMode Enum",
  "The `ProcessingMode` enum defines how import jobs are executed:": "The `ProcessingMode` enum defines how import jobs are executed:",
  "Direct processing without Step Functions": "Direct processing without Step Functions",
  "Processing orchestrated by Step Functions": "Processing orchestrated by Step Functions",
  "Mode": "Mode",
  "Description": "Description",
  "Use Case": "Use Case",
  "Import is processed directly without Step Functions orchestration": "Import is processed directly without Step Functions orchestration",
  "Small imports, simple data": "Small imports, simple data",
  "Import is orchestrated by Step Functions for reliability": "Import is orchestrated by Step Functions for reliability",
  "Large imports, complex workflows, ZIP imports": "Large imports, complex workflows, ZIP imports",
  "CreateCsvImportDto": "CreateCsvImportDto",
  "The `CreateCsvImportDto` is used to start a CSV import job:": "The `CreateCsvImportDto` is used to start a CSV import job:",
  "Optional source identifier": "Optional source identifier",
  "How the import should be processed": "How the import should be processed",
  "S3 bucket containing the CSV file": "S3 bucket containing the CSV file",
  "S3 key (path) to the CSV file": "S3 key (path) to the CSV file",
  "Target table name for import profile matching": "Target table name for import profile matching",
  "Tenant code for multi-tenancy": "Tenant code for multi-tenancy",
  "Property": "Property",
  "Type": "Type",
  "Required": "Required",
  "No": "No",
  "Optional identifier for the import source": "Optional identifier for the import source",
  "Yes": "Yes",
  "DIRECT or STEP_FUNCTION mode": "DIRECT or STEP_FUNCTION mode",
  "Target table name, used to match import profile": "Target table name, used to match import profile",
  "CreateZipImportDto": "CreateZipImportDto",
  "The `CreateZipImportDto` is used to start a ZIP import job that contains multiple CSV files:": "The `CreateZipImportDto` is used to start a ZIP import job that contains multiple CSV files:",
  "S3 bucket containing the ZIP file": "S3 bucket containing the ZIP file",
  "S3 key (path) to the ZIP file": "S3 key (path) to the ZIP file",
  "High priority: sortedFileKeys": "High priority: sortedFileKeys",
  "If not provided, it will use the default sorting logic": "If not provided, it will use the default sorting logic",
  "Optional ordered list of file keys to process": "Optional ordered list of file keys to process",
  "High priority: tableName": "High priority: tableName",
  "If not provided, it will be extracted from the filename": "If not provided, it will be extracted from the filename",
  "Optional table name override": "Optional table name override",
  "Ordered list of file keys to process. If not provided, default sorting is used": "Ordered list of file keys to process. If not provided, default sorting is used",
  "Table name override. If not provided, extracted from filename (format: yyyymmddhhMMss-\\{tableName\\}.csv)": "Table name override. If not provided, extracted from filename (format: yyyymmddhhMMss-\\{tableName\\}.csv)",
  "Core Concepts": "Core Concepts",
  "ComparisonStatus Enum": "ComparisonStatus Enum",
  "The `ComparisonStatus` enum defines the result of comparing imported data with existing data:": "The `ComparisonStatus` enum defines the result of comparing imported data with existing data:",
  "Data exists and is identical - no action needed": "Data exists and is identical - no action needed",
  "Data does not exist - create new record": "Data does not exist - create new record",
  "Data exists but differs - update existing record": "Data exists but differs - update existing record",
  "Value": "Value",
  "Imported data matches existing data": "Imported data matches existing data",
  "Skip (no operation)": "Skip (no operation)",
  "No existing data found": "No existing data found",
  "Create new record": "Create new record",
  "Existing data differs from imported data": "Existing data differs from imported data",
  "Update existing record": "Update existing record",
  "ComparisonResult Interface": "ComparisonResult Interface",
  "The `ComparisonResult<TEntity>` interface wraps the comparison status with optional existing data. The generic type `TEntity` must extend `DataModel`:": "The `ComparisonResult<TEntity>` interface wraps the comparison status with optional existing data. The generic type `TEntity` must extend `DataModel`:",
  "The result of the comparison": "The result of the comparison",
  "If the status is 'CHANGED', this property holds the existing entity data": "If the status is 'CHANGED', this property holds the existing entity data",
  "retrieved from the database. It is undefined otherwise.": "retrieved from the database. It is undefined otherwise.",
  "The comparison result status": "The comparison result status",
  "The existing entity data, present when status is CHANGED": "The existing entity data, present when status is CHANGED",
  "IProcessStrategy Interface": "IProcessStrategy Interface",
  "The `IProcessStrategy` interface defines the contract for processing validated import data. Note that the generic type `TEntity` must extend `DataModel`:": "The `IProcessStrategy` interface defines the contract for processing validated import data. Note that the generic type `TEntity` must extend `DataModel`:",
  "Get the command service for publishing commands": "Get the command service for publishing commands",
  "Compare the validated DTO with existing data": "Compare the validated DTO with existing data",
  "Map the DTO to a command payload based on comparison status": "Map the DTO to a command payload based on comparison status",
  "Note: status excludes EQUAL since no mapping is needed for identical data": "Note: status excludes EQUAL since no mapping is needed for identical data",
  "CommandInputModel for create, CommandPartialInputModel for update": "CommandInputModel for create, CommandPartialInputModel for update",
  "BaseProcessStrategy Abstract Class": "BaseProcessStrategy Abstract Class",
  "The `BaseProcessStrategy` abstract class provides a base implementation that subclasses must extend. The generic type `TEntity` must extend `DataModel`:": "The `BaseProcessStrategy` abstract class provides a base implementation that subclasses must extend. The generic type `TEntity` must extend `DataModel`:",
  "Abstract method - must be implemented to return the command service": "Abstract method - must be implemented to return the command service",
  "Abstract method - must be implemented to compare data": "Abstract method - must be implemented to compare data",
  "Abstract method - must be implemented to map data to command payload": "Abstract method - must be implemented to map data to command payload",
  "Note": "Note",
  "All three methods (`getCommandService()`, `compare()`, `map()`) in `BaseProcessStrategy` are abstract and must be implemented by subclasses.": "All three methods (`getCommandService()`, `compare()`, `map()`) in `BaseProcessStrategy` are abstract and must be implemented by subclasses.",
  "Return CommandInputModel for creating new records": "Return CommandInputModel for creating new records",
  "status === ComparisonStatus.CHANGED": "status === ComparisonStatus.CHANGED",
  "Return CommandPartialInputModel for updating existing records": "Return CommandPartialInputModel for updating existing records",
  "The module operates on a two-phase architecture:": "The module operates on a two-phase architecture:",
  "**Import Phase** (`IImportStrategy`): Transform raw data (from JSON or CSV) into a standardized DTO and validate it.": "**Import Phase** (`IImportStrategy`): Transform raw data (from JSON or CSV) into a standardized DTO and validate it.",
  "**Process Phase** (`IProcessStrategy`): Compare validated DTO with existing data and map it to a command payload for creation or update.": "**Process Phase** (`IProcessStrategy`): Compare validated DTO with existing data and map it to a command payload for creation or update.",
  "Implementing Import Strategy": "Implementing Import Strategy",
  "The import strategy handles initial transformation and validation:": "The import strategy handles initial transformation and validation:",
  "Implementing Process Strategy": "Implementing Process Strategy",
  "The process strategy contains core business logic for comparing and mapping data:": "The process strategy contains core business logic for comparing and mapping data:",
  "Module Configuration": "Module Configuration",
  "Register the ImportModule with your profiles:": "Register the ImportModule with your profiles:",
  "Custom Event Factory for Imports": "Custom Event Factory for Imports",
  "Configure the event factory to handle import events:": "Configure the event factory to handle import events:",
  "ImportStatusHandler API": "ImportStatusHandler API",
  "The `ImportStatusHandler` is an internal event handler that manages Step Functions callbacks for import jobs. When using Step Functions orchestration (ZIP imports or STEP_FUNCTION mode CSV imports), this handler ensures proper communication with the state machine.": "The `ImportStatusHandler` is an internal event handler that manages Step Functions callbacks for import jobs. When using Step Functions orchestration (ZIP imports or STEP_FUNCTION mode CSV imports), this handler ensures proper communication with the state machine.",
  "Behavior": "Behavior",
  "Import Status": "Import Status",
  "Action": "Action",
  "Step Functions Command": "Step Functions Command",
  "Send success callback": "Send success callback",
  "Send failure callback": "Send failure callback",
  "Other statuses": "Other statuses",
  "Ignored": "Ignored",
  "None": "None",
  "Methods": "Methods",
  "Method": "Method",
  "Sends success signal to Step Functions with the import result": "Sends success signal to Step Functions with the import result",
  "Sends failure signal to Step Functions with error details": "Sends failure signal to Step Functions with error details",
  "Step Functions Integration": "Step Functions Integration",
  "When an import job is created as part of a Step Functions workflow (e.g., ZIP import), a `taskToken` is stored in the job's attributes. The `ImportStatusHandler` listens for status change notifications and:": "When an import job is created as part of a Step Functions workflow (e.g., ZIP import), a `taskToken` is stored in the job's attributes. The `ImportStatusHandler` listens for status change notifications and:",
  "Retrieves the import job from DynamoDB": "Retrieves the import job from DynamoDB",
  "Checks if a `taskToken` exists in the job's attributes": "Checks if a `taskToken` exists in the job's attributes",
  "Sends the appropriate callback based on the final status:": "Sends the appropriate callback based on the final status:",
  "with result data": "with result data",
  "with error details": "with error details",
  "This ensures Step Functions workflows properly handle both success and failure cases without hanging indefinitely.": "This ensures Step Functions workflows properly handle both success and failure cases without hanging indefinitely.",
  "The `sendTaskFailure()` method was added in [version 1.0.18](./changelog#v1018) to fix an issue where Step Functions would wait indefinitely when import jobs failed. See also [Import Module Errors](./error-catalog#import-module-errors) for troubleshooting.": "The `sendTaskFailure()` method was added in [version 1.0.18](./changelog#v1018) to fix an issue where Step Functions would wait indefinitely when import jobs failed. See also [Import Module Errors](./error-catalog#import-module-errors) for troubleshooting.",
  "ImportQueueEventHandler Error Handling": "ImportQueueEventHandler Error Handling",
  "The `ImportQueueEventHandler` processes individual import records from the SQS queue. When an error occurs during processing (e.g., `ConditionalCheckFailedException`), the handler properly updates the parent job status.": "The `ImportQueueEventHandler` processes individual import records from the SQS queue. When an error occurs during processing (e.g., `ConditionalCheckFailedException`), the handler properly updates the parent job status.",
  "Error Flow (v1.0.19+)": "Error Flow (v1.0.19+)",
  "Child Job Error Occurs": "Child Job Error Occurs",
  "Mark Child Job as FAILED": "Mark Child Job as FAILED",
  "Update Parent Job Counters": "Update Parent Job Counters",
  "Check if All Children Complete": "Check if All Children Complete",
  "Update Master": "Update Master",
  "more children": "more children",
  "FAILED": "FAILED",
  "COMPLETED": "COMPLETED",
  "ImportStatusHandler Triggered": "ImportStatusHandler Triggered",
  "SendTaskFailure/SendTaskSuccess": "SendTaskFailure/SendTaskSuccess",
  "Key Methods": "Key Methods",
  "Orchestrates single import record processing with error handling": "Orchestrates single import record processing with error handling",
  "Executes compare, map, and save lifecycle for a strategy": "Executes compare, map, and save lifecycle for a strategy",
  "Error Handling Behavior": "Error Handling Behavior",
  "When a child import job fails:": "When a child import job fails:",
  "Child job status is set to `FAILED` with error details": "Child job status is set to `FAILED` with error details",
  "Parent job counters are atomically updated (`failedRows` incremented)": "Parent job counters are atomically updated (`failedRows` incremented)",
  "When all children complete, master job status is set based on results:": "When all children complete, master job status is set based on results:",
  "If `failedRows > 0` → Master status = `FAILED`": "If `failedRows > 0` → Master status = `FAILED`",
  "If all succeeded → Master status = `COMPLETED`": "If all succeeded → Master status = `COMPLETED`",
  "Lambda does NOT crash - error is handled gracefully": "Lambda does NOT crash - error is handled gracefully",
  "`ConditionalCheckFailedException`: This occurs when attempting to import data that already exists with conflicting version. The import job will be marked as FAILED and the parent job will properly aggregate this failure.": "`ConditionalCheckFailedException`: This occurs when attempting to import data that already exists with conflicting version. The import job will be marked as FAILED and the parent job will properly aggregate this failure.",
  "Prior to v1.0.19, errors in child jobs would crash the Lambda and leave the master job in `PROCESSING` status indefinitely. The fixes in [version 1.0.19](./changelog#v1019) ensure proper error propagation and status updates.": "Prior to v1.0.19, errors in child jobs would crash the Lambda and leave the master job in `PROCESSING` status indefinitely. The fixes in [version 1.0.19](./changelog#v1019) ensure proper error propagation and status updates.",
  "CsvImportSfnEventHandler": "CsvImportSfnEventHandler",
  "The `CsvImportSfnEventHandler` handles Step Functions CSV import workflow states. It manages the `csv_loader` and `finalize_parent_job` states in the import state machine.": "The `CsvImportSfnEventHandler` handles Step Functions CSV import workflow states. It manages the `csv_loader` and `finalize_parent_job` states in the import state machine.",
  "Routes events to appropriate handlers based on state name (`csv_loader` or `finalize_parent_job`)": "Routes events to appropriate handlers based on state name (`csv_loader` or `finalize_parent_job`)",
  "Processes the csv_loader state, creates child jobs for CSV rows": "Processes the csv_loader state, creates child jobs for CSV rows",
  "Finalizes the parent job after all children complete, sets final status": "Finalizes the parent job after all children complete, sets final status",
  "Status Determination (v1.0.20+)": "Status Determination (v1.0.20+)",
  "When finalizing the parent job, the handler correctly determines the final status:": "When finalizing the parent job, the handler correctly determines the final status:",
  "Correct behavior (v1.0.20+)": "Correct behavior (v1.0.20+)",
  "Any child failed → FAILED": "Any child failed → FAILED",
  "All children succeeded → COMPLETED": "All children succeeded → COMPLETED",
  "Known Issue (Fixed in v1.0.20)": "Known Issue (Fixed in v1.0.20)",
  "Prior to v1.0.20, a bug in the ternary operator caused the status to always be `COMPLETED`:": "Prior to v1.0.20, a bug in the ternary operator caused the status to always be `COMPLETED`:",
  "Bug (pre-v1.0.20): always returned COMPLETED": "Bug (pre-v1.0.20): always returned COMPLETED",
  "Wrong!": "Wrong!",
  "This caused Step Functions to report SUCCESS even when child import jobs failed. See [version 1.0.20](./changelog#v1020) for details.": "This caused Step Functions to report SUCCESS even when child import jobs failed. See [version 1.0.20](./changelog#v1020) for details.",
  "ZipImportSfnEventHandler": "ZipImportSfnEventHandler",
  "The `ZipImportSfnEventHandler` handles Step Functions ZIP import workflow states. It orchestrates the processing of multiple CSV files extracted from a ZIP archive.": "The `ZipImportSfnEventHandler` handles Step Functions ZIP import workflow states. It orchestrates the processing of multiple CSV files extracted from a ZIP archive.",
  "Workflow States": "Workflow States",
  "State": "State",
  "Triggers a single CSV import job for each file in the ZIP": "Triggers a single CSV import job for each file in the ZIP",
  "Aggregates results from all CSV imports and finalizes the master job": "Aggregates results from all CSV imports and finalizes the master job",
  "Creates a CSV import job with STEP_FUNCTION mode, passing the taskToken for callback": "Creates a CSV import job with STEP_FUNCTION mode, passing the taskToken for callback",
  "Aggregates results from all processed CSV files and updates the ZIP master job status": "Aggregates results from all processed CSV files and updates the ZIP master job status",
  "File Naming Convention": "File Naming Convention",
  "When processing CSV files from a ZIP archive, the handler extracts the table name from the filename:": "When processing CSV files from a ZIP archive, the handler extracts the table name from the filename:",
  "Format: yyyymmddhhMMss-\\{tableName\\}.csv": "Format: yyyymmddhhMMss-\\{tableName\\}.csv",
  "Example: 20240115120000-products.csv → extracts tableName = \"products\"": "Example: 20240115120000-products.csv → extracts tableName = \"products\"",
  "If `tableName` is provided in the `CreateZipImportDto`, it overrides the extracted name.": "If `tableName` is provided in the `CreateZipImportDto`, it overrides the extracted name.",
  "Processing Flow": "Processing Flow",
  "ZIP File Uploaded to S3": "ZIP File Uploaded to S3",
  "Step Functions Triggered": "Step Functions Triggered",
  "Unzip and List CSV Files": "Unzip and List CSV Files",
  "Map State: For Each CSV File": "Map State: For Each CSV File",
  "trigger_single_csv_and_wait": "trigger_single_csv_and_wait",
  "CSV Import Job Created": "CSV Import Job Created",
  "(with taskToken)": "(with taskToken)",
  "Wait for Completion": "Wait for Completion",
  "finalize_zip_job": "finalize_zip_job",
  "Aggregate Results & Update Master Job": "Aggregate Results & Update Master Job",
  "ZipImportSfnEvent Structure": "ZipImportSfnEvent Structure",
  "Execution ID from Step Functions": "Execution ID from Step Functions",
  "Step Functions context with state info": "Step Functions context with state info",
  "S3 key or array of results": "S3 key or array of results",
  "Token for callback to Step Functions": "Token for callback to Step Functions",
  "The `context.Execution.Input` contains:": "The `context.Execution.Input` contains:",
  "Primary key of the ZIP master job in DynamoDB": "Primary key of the ZIP master job in DynamoDB",
  "Original import parameters (bucket, tenantCode, tableName)": "Original import parameters (bucket, tenantCode, tableName)",
  "Related Documentation": "Related Documentation",
  "[Backend Development Guide](./backend-development) - Core backend patterns": "[Backend Development Guide](./backend-development) - Core backend patterns",
  "[Service Patterns](./service-patterns) - Service implementation": "[Service Patterns](./service-patterns) - Service implementation",
  "[Step Functions](./architecture/step-functions) - Workflow orchestration": "[Step Functions](./architecture/step-functions) - Workflow orchestration",
  "[Data Sync Handler Examples](./data-sync-handler-examples) - Sync handler patterns": "[Data Sync Handler Examples](./data-sync-handler-examples) - Sync handler patterns"
}