"use strict";(self.webpackChunkdoc=self.webpackChunkdoc||[]).push([[8305],{7752:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"import-export-patterns","title":"Import/Export Patterns","description":"Learn patterns for importing and exporting data in CSV and Excel formats with batch processing and validation.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/import-export-patterns.md","sourceDirName":".","slug":"/import-export-patterns","permalink":"/docs/import-export-patterns","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":16,"frontMatter":{"sidebar_position":16,"description":"Learn patterns for importing and exporting data in CSV and Excel formats with batch processing and validation."},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Tenant Patterns","permalink":"/docs/multi-tenant-patterns"},"next":{"title":"API Integration Guide","permalink":"/docs/api-integration-guide"}}');var s=t(4848),o=t(8453);const i={sidebar_position:16,description:"Learn patterns for importing and exporting data in CSV and Excel formats with batch processing and validation."},a="Import/Export Patterns",c={},d=[{value:"When to Use This Guide",id:"when-to-use-this-guide",level:2},{value:"Import Architecture Overview",id:"import-architecture-overview",level:2},{value:"File Upload Pattern",id:"file-upload-pattern",level:2},{value:"Storage Service",id:"storage-service",level:3},{value:"Storage Controller",id:"storage-controller",level:3},{value:"CSV Import Pattern",id:"csv-import-pattern",level:2},{value:"CSV Import Controller",id:"csv-import-controller",level:3},{value:"CSV Parser Service",id:"csv-parser-service",level:3},{value:"Import Event Handler",id:"import-event-handler",level:3},{value:"Excel Import Pattern",id:"excel-import-pattern",level:2},{value:"Excel Helper Functions",id:"excel-helper-functions",level:3},{value:"Excel Import Service",id:"excel-import-service",level:3},{value:"Import Strategy Pattern",id:"import-strategy-pattern",level:3},{value:"Concrete Import Strategy",id:"concrete-import-strategy",level:3},{value:"Export Pattern",id:"export-pattern",level:2},{value:"Export Service",id:"export-service",level:3},{value:"Step Function Integration",id:"step-function-integration",level:2},{value:"Import Orchestration",id:"import-orchestration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Batch Processing",id:"1-batch-processing",level:3},{value:"2. Error Handling",id:"2-error-handling",level:3},{value:"3. Validation Before Processing",id:"3-validation-before-processing",level:3},{value:"4. Progress Reporting",id:"4-progress-reporting",level:3},{value:"ImportModule API Reference",id:"importmodule-api-reference",level:2},{value:"Installation",id:"installation",level:3},{value:"ProcessingMode Enum",id:"processingmode-enum",level:3},{value:"CreateCsvImportDto",id:"createcsvimportdto",level:3},{value:"CreateZipImportDto",id:"createzipimportdto",level:3},{value:"Core Concepts",id:"core-concepts",level:3},{value:"Implementing Import Strategy",id:"implementing-import-strategy",level:3},{value:"ComparisonStatus Enum",id:"comparisonstatus-enum",level:3},{value:"ComparisonResult Interface",id:"comparisonresult-interface",level:3},{value:"IProcessStrategy Interface",id:"iprocessstrategy-interface",level:3},{value:"BaseProcessStrategy Abstract Class",id:"baseprocessstrategy-class",level:3},{value:"Implementing Process Strategy",id:"implementing-process-strategy",level:3},{value:"Module Configuration",id:"module-configuration",level:3},{value:"Custom Event Factory for Imports",id:"custom-event-factory-for-imports",level:3},{value:"ImportStatusHandler API",id:"importstatushandler-api",level:3},{value:"Behavior",id:"behavior",level:4},{value:"Methods",id:"methods",level:4},{value:"Step Functions Integration",id:"step-functions-integration",level:4},{value:"ImportQueueEventHandler Error Handling",id:"import-error-handling",level:3},{value:"Error Flow (v1.0.19+)",id:"error-flow-v1019",level:4},{value:"Key Methods",id:"key-methods",level:4},{value:"Error Handling Behavior",id:"error-handling-behavior",level:4},{value:"CsvImportSfnEventHandler",id:"csvimportsfneventhandler",level:3},{value:"Key Methods",id:"key-methods-1",level:4},{value:"Status Determination (v1.0.20+)",id:"status-determination-v1020",level:4},{value:"ZipImportSfnEventHandler",id:"zipimportsfneventhandler",level:3},{value:"Workflow States",id:"workflow-states",level:4},{value:"Key Methods",id:"key-methods-2",level:4},{value:"File Naming Convention",id:"file-naming-convention",level:4},{value:"Processing Flow",id:"processing-flow",level:4},{value:"ZipImportSfnEvent Structure",id:"zipimportsfnevent-structure",level:4},{value:"Related Documentation",id:"related-documentation",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"importexport-patterns",children:"Import/Export Patterns"})}),"\n",(0,s.jsx)(n.p,{children:"This guide covers patterns for handling data import and export operations, including CSV processing, Excel file handling, and batch data operations with Step Functions."}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use-this-guide",children:"When to Use This Guide"}),"\n",(0,s.jsx)(n.p,{children:"Use this guide when you need to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Import bulk data from CSV or Excel files"}),"\n",(0,s.jsx)(n.li,{children:"Export data to various formats"}),"\n",(0,s.jsx)(n.li,{children:"Process large datasets with Step Functions"}),"\n",(0,s.jsx)(n.li,{children:"Implement file upload with S3 presigned URLs"}),"\n",(0,s.jsx)(n.li,{children:"Transform data between external and internal formats"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"import-architecture-overview",children:"Import Architecture Overview"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500>\u2502     S3      \u2502\u2500\u2500\u2500\u2500>\u2502Step Function\u2502\u2500\u2500\u2500\u2500>\u2502   Lambda    \u2502\n\u2502  (Upload)   \u2502     \u2502  (Storage)  \u2502     \u2502(Orchestrate)\u2502     \u2502 (Process)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                    \u2502\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  DynamoDB   \u2502<\u2500\u2500\u2500\u2500\u2502   Import    \u2502\n                    \u2502  (Command)  \u2502     \u2502   Handler   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"file-upload-pattern",children:"File Upload Pattern"}),"\n",(0,s.jsx)(n.h3,{id:"storage-service",children:"Storage Service"}),"\n",(0,s.jsx)(n.p,{children:"Generate presigned URLs for secure file uploads:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// storage/storage.service.ts\nimport { Injectable } from '@nestjs/common';\nimport { S3Service } from '@mbc-cqrs-serverless/core';\nimport { getSignedUrl } from '@aws-sdk/s3-request-presigner';\nimport { PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';\n\n@Injectable()\nexport class StorageService {\n  constructor(private readonly s3Service: S3Service) {}\n\n  /**\n   * Generate upload URL for file import\n   */\n  async genUploadUrl(\n    filename: string,\n    contentType = 'text/csv',\n  ): Promise<{ bucket: string; key: string; url: string }> {\n    const bucket = this.s3Service.privateBucket;\n    const timestamp = Date.now();\n    const key = `imports/${timestamp}/${filename}`;\n\n    const command = new PutObjectCommand({\n      Bucket: bucket,\n      Key: key,\n      ContentType: contentType,\n      ACL: 'private',\n    });\n\n    const url = await getSignedUrl(this.s3Service.client, command, {\n      expiresIn: 3600, // 1 hour\n    });\n\n    return { bucket, key, url };\n  }\n\n  /**\n   * Generate download URL for file export\n   */\n  async genDownloadUrl(\n    key: string,\n    filename?: string,\n  ): Promise<{ url: string }> {\n    const command = new GetObjectCommand({\n      Bucket: this.s3Service.privateBucket,\n      Key: key,\n      ResponseContentDisposition: filename\n        ? `attachment; filename=\"${filename}\"`\n        : undefined,\n    });\n\n    const url = await getSignedUrl(this.s3Service.client, command, {\n      expiresIn: 3600,\n    });\n\n    return { url };\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"storage-controller",children:"Storage Controller"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// storage/storage.controller.ts\nimport { Controller, Post, Get, Body, Query } from '@nestjs/common';\nimport { StorageService } from './storage.service';\n\n@Controller('api/storage')\nexport class StorageController {\n  constructor(private readonly storageService: StorageService) {}\n\n  /**\n   * Get presigned URL for upload\n   */\n  @Post('upload-url')\n  async getUploadUrl(\n    @Body() dto: { filename: string; contentType?: string },\n  ) {\n    return this.storageService.genUploadUrl(dto.filename, dto.contentType);\n  }\n\n  /**\n   * Get presigned URL for download\n   */\n  @Get('download-url')\n  async getDownloadUrl(\n    @Query('key') key: string,\n    @Query('filename') filename?: string,\n  ) {\n    return this.storageService.genDownloadUrl(key, filename);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"csv-import-pattern",children:"CSV Import Pattern"}),"\n",(0,s.jsx)(n.h3,{id:"csv-import-controller",children:"CSV Import Controller"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// csv-import/csv-import.controller.ts\nimport { Controller, Post, Body } from '@nestjs/common';\nimport { ConfigService } from '@nestjs/config';\nimport { StepFunctionService, INVOKE_CONTEXT, IInvoke } from '@mbc-cqrs-serverless/core';\n\nexport class CsvImportDto {\n  bucket: string;\n  key: string;\n  type: string;  // Import type identifier\n}\n\n@Controller('api/csv-import')\nexport class CsvImportController {\n  private readonly importArn: string;\n\n  constructor(\n    private readonly configService: ConfigService,\n    private readonly sfnService: StepFunctionService,\n  ) {\n    this.importArn = this.configService.get<string>('SFN_CSV_IMPORT_ARN');\n  }\n\n  /**\n   * Start CSV import via Step Functions\n   */\n  @Post('/')\n  async startImport(\n    @INVOKE_CONTEXT() invokeContext: IInvoke,\n    @Body() dto: CsvImportDto,\n  ) {\n    const executionName = `${dto.type}-${Date.now()}`;\n\n    return this.sfnService.startExecution(\n      this.importArn,\n      {\n        ...dto,\n        invokeContext,\n      },\n      executionName,\n    );\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"csv-parser-service",children:"CSV Parser Service"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// csv-import/csv-parser.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { S3Service } from '@mbc-cqrs-serverless/core';\nimport { GetObjectCommand } from '@aws-sdk/client-s3';\nimport * as csvParser from 'csv-parser';\nimport { Readable } from 'stream';\n\nexport interface ParsedRow {\n  rowNumber: number;\n  data: Record<string, string>;\n  errors: string[];\n}\n\n@Injectable()\nexport class CsvParserService {\n  private readonly logger = new Logger(CsvParserService.name);\n\n  constructor(private readonly s3Service: S3Service) {}\n\n  /**\n   * Parse CSV file from S3\n   */\n  async parseFromS3(\n    bucket: string,\n    key: string,\n    options?: { encoding?: string; delimiter?: string },\n  ): Promise<ParsedRow[]> {\n    const command = new GetObjectCommand({ Bucket: bucket, Key: key });\n    const response = await this.s3Service.client.send(command);\n\n    const stream = response.Body as Readable;\n    const results: ParsedRow[] = [];\n    let rowNumber = 0;\n\n    return new Promise((resolve, reject) => {\n      stream\n        .pipe(csvParser({\n          separator: options?.delimiter || ',',\n          skipLines: 0,\n        }))\n        .on('data', (data) => {\n          rowNumber++;\n          results.push({\n            rowNumber,\n            data,\n            errors: [],\n          });\n        })\n        .on('end', () => {\n          this.logger.log(`Parsed ${results.length} rows from ${key}`);\n          resolve(results);\n        })\n        .on('error', (error) => {\n          this.logger.error(`Failed to parse CSV: ${error.message}`);\n          reject(error);\n        });\n    });\n  }\n\n  /**\n   * Validate parsed rows\n   */\n  validateRows(\n    rows: ParsedRow[],\n    requiredFields: string[],\n  ): ParsedRow[] {\n    return rows.map(row => {\n      const errors: string[] = [];\n\n      for (const field of requiredFields) {\n        if (!row.data[field] || row.data[field].trim() === '') {\n          errors.push(`Missing required field: ${field}`);\n        }\n      }\n\n      return { ...row, errors };\n    });\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"import-event-handler",children:"Import Event Handler"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// csv-import/event/csv-import.event.handler.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { EventHandler, IEventHandler, SNSService } from '@mbc-cqrs-serverless/core';\nimport { ConfigService } from '@nestjs/config';\nimport { CsvParserService } from '../csv-parser.service';\nimport { ProductService } from '../../product/product.service';\n\nexport class CsvImportEvent {\n  bucket: string;\n  key: string;\n  type: string;\n  invokeContext: any;\n}\n\n@EventHandler(CsvImportEvent)\n@Injectable()\nexport class CsvImportEventHandler implements IEventHandler<CsvImportEvent> {\n  private readonly logger = new Logger(CsvImportEventHandler.name);\n  private readonly alarmTopicArn: string;\n\n  constructor(\n    private readonly csvParser: CsvParserService,\n    private readonly productService: ProductService,\n    private readonly snsService: SNSService,\n    private readonly configService: ConfigService,\n  ) {\n    this.alarmTopicArn = this.configService.get<string>('SNS_ALARM_TOPIC_ARN');\n  }\n\n  /**\n   * Process CSV import event\n   */\n  async execute(event: CsvImportEvent): Promise<any> {\n    this.logger.log(`Processing import: ${event.key}`);\n\n    try {\n      // Parse CSV\n      const rows = await this.csvParser.parseFromS3(event.bucket, event.key);\n\n      // Validate\n      const validatedRows = this.csvParser.validateRows(rows, [\n        'code', 'name', 'price',\n      ]);\n\n      // Filter valid rows\n      const validRows = validatedRows.filter(r => r.errors.length === 0);\n      const invalidRows = validatedRows.filter(r => r.errors.length > 0);\n\n      if (invalidRows.length > 0) {\n        this.logger.warn(`${invalidRows.length} rows have validation errors`);\n      }\n\n      // Process in batches\n      const batchSize = 30;\n      let processedCount = 0;\n\n      for (let i = 0; i < validRows.length; i += batchSize) {\n        const batch = validRows.slice(i, i + batchSize);\n\n        await Promise.all(\n          batch.map(row => this.processRow(row.data, event.invokeContext)),\n        );\n\n        processedCount += batch.length;\n        this.logger.log(`Processed ${processedCount}/${validRows.length} rows`);\n      }\n\n      return {\n        success: true,\n        totalRows: rows.length,\n        processedRows: processedCount,\n        errorRows: invalidRows.length,\n      };\n    } catch (error) {\n      await this.sendAlarm(event, error);\n      throw error;\n    }\n  }\n\n  private async processRow(data: Record<string, string>, invokeContext: any) {\n    await this.productService.publishCommand({\n      code: data.code,\n      name: data.name,\n      attributes: {\n        price: parseFloat(data.price),\n        category: data.category,\n        description: data.description,\n      },\n    }, invokeContext);\n  }\n\n  private async sendAlarm(event: CsvImportEvent, error: Error) {\n    await this.snsService.publish({\n      topicArn: this.alarmTopicArn,\n      subject: 'CSV Import Error',\n      message: JSON.stringify({\n        key: event.key,\n        error: error.message,\n        timestamp: new Date().toISOString(),\n      }),\n    });\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"excel-import-pattern",children:"Excel Import Pattern"}),"\n",(0,s.jsx)(n.h3,{id:"excel-helper-functions",children:"Excel Helper Functions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// helpers/excel.ts\nimport { Workbook, Worksheet, Cell } from 'exceljs';\n\n/**\n * Get cell value handling formulas and rich text\n */\nexport function getCellValue(row: any, column: string): string | undefined {\n  const cell = row.getCell(column);\n\n  if (!cell || cell.value === null || cell.value === undefined) {\n    return undefined;\n  }\n\n  // Handle formula result\n  if (typeof cell.value === 'object' && 'result' in cell.value) {\n    return String(cell.value.result);\n  }\n\n  // Handle rich text\n  if (typeof cell.value === 'object' && 'richText' in cell.value) {\n    return cell.value.richText.map((r: any) => r.text).join('');\n  }\n\n  return String(cell.value);\n}\n\n/**\n * Get numeric cell value\n */\nexport function getCellNumber(row: any, column: string): number | undefined {\n  const value = getCellValue(row, column);\n  if (!value) return undefined;\n\n  const num = parseFloat(value.replace(/,/g, ''));\n  return isNaN(num) ? undefined : num;\n}\n\n/**\n * Get date cell value\n */\nexport function getCellDate(row: any, column: string): Date | undefined {\n  const cell = row.getCell(column);\n\n  if (cell.value instanceof Date) {\n    return cell.value;\n  }\n\n  const value = getCellValue(row, column);\n  if (!value) return undefined;\n\n  const date = new Date(value);\n  return isNaN(date.getTime()) ? undefined : date;\n}\n\n/**\n * Find header row by matching column headers\n */\nexport function findHeaderRow(\n  worksheet: Worksheet,\n  expectedHeaders: string[],\n  maxRows = 20,\n): number {\n  for (let rowNum = 1; rowNum <= maxRows; rowNum++) {\n    const row = worksheet.getRow(rowNum);\n    const values = row.values as any[];\n\n    const matches = expectedHeaders.filter(header =>\n      values.some(v => v && String(v).includes(header)),\n    );\n\n    if (matches.length >= expectedHeaders.length * 0.8) {\n      return rowNum;\n    }\n  }\n\n  throw new Error('Header row not found');\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"excel-import-service",children:"Excel Import Service"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// excel-import/excel-import.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { S3Service } from '@mbc-cqrs-serverless/core';\nimport { GetObjectCommand } from '@aws-sdk/client-s3';\nimport { Workbook } from 'exceljs';\nimport { getCellValue, getCellNumber, findHeaderRow } from '../helpers/excel';\n\nexport interface ExcelImportResult {\n  success: boolean;\n  sheetName: string;\n  totalRows: number;\n  processedRows: number;\n  errors: Array<{ row: number; message: string }>;\n}\n\n@Injectable()\nexport class ExcelImportService {\n  private readonly logger = new Logger(ExcelImportService.name);\n\n  constructor(private readonly s3Service: S3Service) {}\n\n  /**\n   * Load workbook from S3\n   */\n  async loadWorkbook(bucket: string, key: string): Promise<Workbook> {\n    const command = new GetObjectCommand({ Bucket: bucket, Key: key });\n    const response = await this.s3Service.client.send(command);\n\n    const chunks: Buffer[] = [];\n    for await (const chunk of response.Body as any) {\n      chunks.push(chunk);\n    }\n    const buffer = Buffer.concat(chunks);\n\n    const workbook = new Workbook();\n\n    if (key.endsWith('.xlsx') || key.endsWith('.xlsm')) {\n      await workbook.xlsx.load(buffer);\n    } else if (key.endsWith('.xls')) {\n      // For .xls files, use different parser\n      throw new Error('XLS format not supported. Please use XLSX.');\n    }\n\n    return workbook;\n  }\n\n  /**\n   * Process worksheet with row processor\n   */\n  async processWorksheet<T>(\n    worksheet: any,\n    config: {\n      headerRow?: number;\n      expectedHeaders?: string[];\n      startRow?: number;\n      processor: (row: any, rowNumber: number) => Promise<T | null>;\n    },\n  ): Promise<ExcelImportResult & { data: T[] }> {\n    const errors: Array<{ row: number; message: string }> = [];\n    const data: T[] = [];\n\n    // Find or use specified header row\n    const headerRow = config.headerRow || (\n      config.expectedHeaders\n        ? findHeaderRow(worksheet, config.expectedHeaders)\n        : 1\n    );\n\n    const startRow = config.startRow || headerRow + 1;\n    let processedRows = 0;\n    let totalRows = 0;\n\n    worksheet.eachRow((row: any, rowNumber: number) => {\n      if (rowNumber < startRow) return;\n      totalRows++;\n    });\n\n    for (let rowNum = startRow; rowNum <= worksheet.rowCount; rowNum++) {\n      const row = worksheet.getRow(rowNum);\n\n      // Skip empty rows\n      if (this.isEmptyRow(row)) continue;\n\n      try {\n        const result = await config.processor(row, rowNum);\n        if (result !== null) {\n          data.push(result);\n          processedRows++;\n        }\n      } catch (error) {\n        errors.push({\n          row: rowNum,\n          message: error.message,\n        });\n      }\n    }\n\n    return {\n      success: errors.length === 0,\n      sheetName: worksheet.name,\n      totalRows,\n      processedRows,\n      errors,\n      data,\n    };\n  }\n\n  private isEmptyRow(row: any): boolean {\n    const values = row.values as any[];\n    return !values || values.every(v => v === null || v === undefined || v === '');\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"import-strategy-pattern",children:"Import Strategy Pattern"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// import/base-import.strategy.ts\nimport { IInvoke } from '@mbc-cqrs-serverless/core';\n\n/**\n * Base interface for import strategies\n * @typeParam TInput - The input type, must be an object\n * @typeParam TAttributesDto - The output DTO type, must be an object\n */\nexport interface IImportStrategy<TInput extends object, TAttributesDto extends object> {\n  /**\n   * Transform raw input to command DTO\n   */\n  transform(input: TInput): Promise<TAttributesDto>;\n\n  /**\n   * Validate transformed DTO\n   */\n  validate(data: TAttributesDto): Promise<void>;\n}\n\n/**\n * Base import strategy with common functionality\n * @typeParam TInput - The input type, must be an object\n * @typeParam TAttributesDto - The output DTO type, must be an object\n */\nexport abstract class BaseImportStrategy<TInput extends object, TAttributesDto extends object>\n  implements IImportStrategy<TInput, TAttributesDto>\n{\n  /**\n   * Transform raw input to command DTO (default: return as-is)\n   */\n  async transform(input: TInput): Promise<TAttributesDto> {\n    return input as unknown as TAttributesDto;\n  }\n\n  /**\n   * Validate transformed DTO using class-validator\n   */\n  async validate(data: TAttributesDto): Promise<void> {\n    // Uses class-validator for validation\n    const errors = await validate(data as object);\n    if (errors.length > 0) {\n      throw new BadRequestException({\n        statusCode: 400,\n        message: this.flattenValidationErrors(errors),\n        error: 'Bad Request',\n      });\n    }\n  }\n\n  /**\n   * Flatten validation errors to a simple format\n   */\n  private flattenValidationErrors(\n    errors: ValidationError[],\n    parentPath = '',\n  ): string[] {\n    const messages: string[] = [];\n    for (const error of errors) {\n      const currentPath = parentPath\n        ? `${parentPath}.${error.property}`\n        : error.property;\n\n      if (error.children && error.children.length > 0) {\n        messages.push(\n          ...this.flattenValidationErrors(error.children, currentPath),\n        );\n      } else if (error.constraints) {\n        const firstConstraint = Object.values(error.constraints)[0];\n        const message = firstConstraint.replace(error.property, currentPath);\n        messages.push(message);\n      }\n    }\n    return messages;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"concrete-import-strategy",children:"Concrete Import Strategy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// product/import/product-import.strategy.ts\nimport { Injectable } from '@nestjs/common';\nimport { KEY_SEPARATOR, generateId } from '@mbc-cqrs-serverless/core';\nimport { BaseImportStrategy } from '@mbc-cqrs-serverless/import';\nimport { ulid } from 'ulid';\nimport { ProductCommandDto } from '../dto/product-command.dto';\n\nconst PRODUCT_PK_PREFIX = 'PRODUCT';\n\nexport interface ProductImportInput {\n  code: string;\n  name: string;\n  category?: string;\n  price?: string;\n  description?: string;\n  tenantCode: string; // Passed from import context\n}\n\n@Injectable()\nexport class ProductImportStrategy\n  extends BaseImportStrategy<ProductImportInput, ProductCommandDto>\n{\n  /**\n   * Transform import data to command DTO\n   */\n  async transform(input: ProductImportInput): Promise<ProductCommandDto> {\n    const { tenantCode } = input;\n\n    const pk = `${PRODUCT_PK_PREFIX}${KEY_SEPARATOR}${tenantCode}`;\n    const sk = ulid();\n    const id = generateId(pk, sk);\n\n    return new ProductCommandDto({\n      pk,\n      sk,\n      id,\n      tenantCode,\n      code: input.code?.trim(),\n      name: input.name?.trim(),\n      type: 'PRODUCT',\n      attributes: {\n        category: input.category?.trim(),\n        price: input.price ? parseFloat(input.price.replace(/,/g, '')) : undefined,\n        description: input.description?.trim(),\n      },\n    });\n  }\n\n  /**\n   * Validate import input\n   */\n  async validate(input: ProductImportInput): Promise<void> {\n    if (!input.code) {\n      throw new Error('Product code is required');\n    }\n    if (!input.name) {\n      throw new Error('Product name is required');\n    }\n    if (input.price && isNaN(parseFloat(input.price.replace(/,/g, '')))) {\n      throw new Error('Invalid price format');\n    }\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"export-pattern",children:"Export Pattern"}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.p,{children:["The export patterns shown below are example implementations for your application. Unlike the import module (",(0,s.jsx)(n.code,{children:"@mbc-cqrs-serverless/import"}),"), there is no dedicated export package in the framework. You can implement these patterns directly in your application code."]})}),"\n",(0,s.jsx)(n.h3,{id:"export-service",children:"Export Service"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// export/export.service.ts\nimport { Injectable, Logger } from '@nestjs/common';\nimport { S3Service } from '@mbc-cqrs-serverless/core';\nimport { PutObjectCommand } from '@aws-sdk/client-s3';\nimport { Workbook } from 'exceljs';\n\n@Injectable()\nexport class ExportService {\n  private readonly logger = new Logger(ExportService.name);\n\n  constructor(private readonly s3Service: S3Service) {}\n\n  /**\n   * Export data to CSV and upload to S3\n   */\n  async exportToCsv(\n    data: Record<string, any>[],\n    headers: { key: string; label: string }[],\n    filename: string,\n  ): Promise<{ bucket: string; key: string }> {\n    // Build CSV content\n    const headerRow = headers.map(h => h.label).join(',');\n    const dataRows = data.map(row =>\n      headers.map(h => this.escapeCsvValue(row[h.key])).join(','),\n    );\n    const csvContent = [headerRow, ...dataRows].join('\\n');\n\n    // Upload to S3\n    const bucket = this.s3Service.privateBucket;\n    const key = `exports/${Date.now()}/${filename}`;\n\n    await this.s3Service.client.send(new PutObjectCommand({\n      Bucket: bucket,\n      Key: key,\n      Body: csvContent,\n      ContentType: 'text/csv; charset=utf-8',\n    }));\n\n    this.logger.log(`Exported ${data.length} rows to ${key}`);\n    return { bucket, key };\n  }\n\n  /**\n   * Export data to Excel and upload to S3\n   */\n  async exportToExcel(\n    data: Record<string, any>[],\n    headers: { key: string; label: string; width?: number }[],\n    filename: string,\n    sheetName = 'Data',\n  ): Promise<{ bucket: string; key: string }> {\n    const workbook = new Workbook();\n    const worksheet = workbook.addWorksheet(sheetName);\n\n    // Set columns\n    worksheet.columns = headers.map(h => ({\n      header: h.label,\n      key: h.key,\n      width: h.width || 15,\n    }));\n\n    // Style header row\n    worksheet.getRow(1).font = { bold: true };\n    worksheet.getRow(1).fill = {\n      type: 'pattern',\n      pattern: 'solid',\n      fgColor: { argb: 'FFE0E0E0' },\n    };\n\n    // Add data rows\n    data.forEach(row => {\n      const rowData: Record<string, any> = {};\n      headers.forEach(h => {\n        rowData[h.key] = row[h.key];\n      });\n      worksheet.addRow(rowData);\n    });\n\n    // Generate buffer\n    const buffer = await workbook.xlsx.writeBuffer();\n\n    // Upload to S3\n    const bucket = this.s3Service.privateBucket;\n    const key = `exports/${Date.now()}/${filename}`;\n\n    await this.s3Service.client.send(new PutObjectCommand({\n      Bucket: bucket,\n      Key: key,\n      Body: buffer as Buffer,\n      ContentType: 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n    }));\n\n    this.logger.log(`Exported ${data.length} rows to ${key}`);\n    return { bucket, key };\n  }\n\n  private escapeCsvValue(value: any): string {\n    if (value === null || value === undefined) return '';\n\n    const str = String(value);\n    if (str.includes(',') || str.includes('\"') || str.includes('\\n')) {\n      return `\"${str.replace(/\"/g, '\"\"')}\"`;\n    }\n    return str;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-function-integration",children:"Step Function Integration"}),"\n",(0,s.jsx)(n.h3,{id:"import-orchestration",children:"Import Orchestration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'// Import workflow with Step Functions\n\n// serverless.yml\n/*\nstepFunctions:\n  stateMachines:\n    csvImport:\n      name: ${self:custom.prefix}-csv-import\n      definition:\n        StartAt: ParseFile\n        States:\n          ParseFile:\n            Type: Task\n            Resource: !GetAtt ParseFileLambda.Arn\n            Next: ProcessBatches\n            Catch:\n              - ErrorEquals: ["States.ALL"]\n                Next: HandleError\n\n          ProcessBatches:\n            Type: Map\n            ItemsPath: $.batches\n            MaxConcurrency: 5\n            Iterator:\n              StartAt: ProcessBatch\n              States:\n                ProcessBatch:\n                  Type: Task\n                  Resource: !GetAtt ProcessBatchLambda.Arn\n                  End: true\n            Next: Finalize\n\n          Finalize:\n            Type: Task\n            Resource: !GetAtt FinalizeLambda.Arn\n            End: true\n\n          HandleError:\n            Type: Task\n            Resource: !GetAtt HandleErrorLambda.Arn\n            End: true\n*/\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"1-batch-processing",children:"1. Batch Processing"}),"\n",(0,s.jsx)(n.p,{children:"Always process large files in batches:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"const BATCH_SIZE = 30;\n\nasync processBatches<T>(\n  items: T[],\n  processor: (item: T) => Promise<void>,\n): Promise<void> {\n  for (let i = 0; i < items.length; i += BATCH_SIZE) {\n    const batch = items.slice(i, i + BATCH_SIZE);\n    await Promise.all(batch.map(processor));\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-error-handling",children:"2. Error Handling"}),"\n",(0,s.jsx)(n.p,{children:"Collect and report errors without stopping processing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"const errors: Array<{ row: number; error: string }> = [];\n\nfor (const [index, row] of rows.entries()) {\n  try {\n    await processRow(row);\n  } catch (error) {\n    errors.push({ row: index + 1, error: error.message });\n    // Continue processing\n  }\n}\n\nif (errors.length > 0) {\n  await this.reportErrors(errors);\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-validation-before-processing",children:"3. Validation Before Processing"}),"\n",(0,s.jsx)(n.p,{children:"Validate all data before starting import:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// First pass: validate\nconst validationErrors = await this.validateAll(rows);\nif (validationErrors.length > 0) {\n  return { success: false, errors: validationErrors };\n}\n\n// Second pass: process\nawait this.processAll(rows);\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-progress-reporting",children:"4. Progress Reporting"}),"\n",(0,s.jsx)(n.p,{children:"Report progress for long-running imports:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"const total = rows.length;\nlet processed = 0;\n\nfor (const batch of batches) {\n  await processBatch(batch);\n  processed += batch.length;\n\n  // Report progress every 100 rows\n  if (processed % 100 === 0) {\n    this.logger.log(`Progress: ${processed}/${total} (${Math.round(processed/total*100)}%)`);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"importmodule-api-reference",children:"ImportModule API Reference"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"@mbc-cqrs-serverless/import"})," package provides a comprehensive framework for managing data import tasks."]}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"npm install @mbc-cqrs-serverless/import\n"})}),"\n",(0,s.jsx)(n.h3,{id:"processingmode-enum",children:"ProcessingMode Enum"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ProcessingMode"})," enum defines how import jobs are executed:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"export enum ProcessingMode {\n  DIRECT = 'DIRECT',           // Direct processing without Step Functions\n  STEP_FUNCTION = 'STEP_FUNCTION', // Processing orchestrated by Step Functions\n}\n"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Mode"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"DIRECT"})}),(0,s.jsx)(n.td,{children:"Import is processed directly without Step Functions orchestration"}),(0,s.jsx)(n.td,{children:"Small imports, simple data"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"STEP_FUNCTION"})}),(0,s.jsx)(n.td,{children:"Import is orchestrated by Step Functions for reliability"}),(0,s.jsx)(n.td,{children:"Large imports, complex workflows, ZIP imports"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"createcsvimportdto",children:"CreateCsvImportDto"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"CreateCsvImportDto"})," is used to start a CSV import job:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { IsEnum, IsNotEmpty, IsOptional, IsString } from 'class-validator'\nimport { ProcessingMode } from '@mbc-cqrs-serverless/import'\n\nexport class CreateCsvImportDto {\n  @IsString()\n  @IsOptional()\n  sourceId?: string           // Optional source identifier\n\n  @IsNotEmpty()\n  @IsEnum(ProcessingMode)\n  processingMode: ProcessingMode // How the import should be processed\n\n  @IsString()\n  @IsNotEmpty()\n  bucket: string              // S3 bucket containing the CSV file\n\n  @IsString()\n  @IsNotEmpty()\n  key: string                 // S3 key (path) to the CSV file\n\n  @IsString()\n  @IsNotEmpty()\n  tableName: string           // Target table name for import profile matching\n\n  @IsString()\n  @IsNotEmpty()\n  tenantCode: string          // Tenant code for multi-tenancy\n}\n"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Property"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"sourceId"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"Optional identifier for the import source"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"processingMode"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ProcessingMode"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"DIRECT or STEP_FUNCTION mode"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"bucket"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"S3 bucket containing the CSV file"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"key"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"S3 key (path) to the CSV file"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"tableName"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"Target table name, used to match import profile"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"tenantCode"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"Tenant code for multi-tenancy"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"createzipimportdto",children:"CreateZipImportDto"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"CreateZipImportDto"})," is used to start a ZIP import job that contains multiple CSV files:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { IsArray, IsNotEmpty, IsOptional, IsString } from 'class-validator'\n\nexport class CreateZipImportDto {\n  @IsString()\n  @IsNotEmpty()\n  bucket: string              // S3 bucket containing the ZIP file\n\n  @IsString()\n  @IsNotEmpty()\n  key: string                 // S3 key (path) to the ZIP file\n\n  @IsString()\n  @IsNotEmpty()\n  tenantCode: string          // Tenant code for multi-tenancy\n\n  // High priority: sortedFileKeys\n  // If not provided, it will use the default sorting logic\n  @IsArray()\n  @IsOptional()\n  sortedFileKeys?: string[]   // Optional ordered list of file keys to process\n\n  // High priority: tableName\n  // If not provided, it will be extracted from the filename\n  @IsString()\n  @IsOptional()\n  tableName?: string = null   // Optional table name override\n}\n"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Property"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"bucket"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"S3 bucket containing the ZIP file"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"key"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"S3 key (path) to the ZIP file"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"tenantCode"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"Tenant code for multi-tenancy"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"sortedFileKeys"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string[]"})}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"Ordered list of file keys to process. If not provided, default sorting is used"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"tableName"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"Table name override. If not provided, extracted from filename (format: yyyymmddhhMMss-{tableName}.csv)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.p,{children:"The module operates on a two-phase architecture:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Import Phase"})," (",(0,s.jsx)(n.code,{children:"IImportStrategy"}),"): Transform raw data (from JSON or CSV) into a standardized DTO and validate it."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Process Phase"})," (",(0,s.jsx)(n.code,{children:"IProcessStrategy"}),"): Compare validated DTO with existing data and map it to a command payload for creation or update."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementing-import-strategy",children:"Implementing Import Strategy"}),"\n",(0,s.jsx)(n.p,{children:"The import strategy handles initial transformation and validation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { BadRequestException, Injectable } from '@nestjs/common';\nimport { plainToInstance } from 'class-transformer';\nimport { BaseImportStrategy, IImportStrategy } from '@mbc-cqrs-serverless/import';\nimport { PolicyCommandDto } from '../dto/policy-command.dto';\n\n@Injectable()\nexport class PolicyImportStrategy\n  extends BaseImportStrategy<Record<string, any>, PolicyCommandDto>\n  implements IImportStrategy<Record<string, any>, PolicyCommandDto>\n{\n  async transform(input: Record<string, any>): Promise<PolicyCommandDto> {\n    const attrSource = input.attributes && typeof input.attributes === 'object'\n      ? input.attributes\n      : input;\n\n    const mappedObject = {\n      pk: input.pk,\n      sk: input.sk,\n      attributes: {\n        policyType: attrSource.policyType,\n        applyDate: new Date(attrSource.applyDate).toISOString(),\n      },\n    };\n\n    return plainToInstance(PolicyCommandDto, mappedObject);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"comparisonstatus-enum",children:"ComparisonStatus Enum"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ComparisonStatus"})," enum defines the result of comparing imported data with existing data:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"export enum ComparisonStatus {\n  EQUAL = 'EQUAL',         // Data exists and is identical - no action needed\n  NOT_EXIST = 'NOT_EXIST', // Data does not exist - create new record\n  CHANGED = 'CHANGED',     // Data exists but differs - update existing record\n}\n"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Value"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Action"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"EQUAL"})}),(0,s.jsx)(n.td,{children:"Imported data matches existing data"}),(0,s.jsx)(n.td,{children:"Skip (no operation)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NOT_EXIST"})}),(0,s.jsx)(n.td,{children:"No existing data found"}),(0,s.jsx)(n.td,{children:"Create new record"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"CHANGED"})}),(0,s.jsx)(n.td,{children:"Existing data differs from imported data"}),(0,s.jsx)(n.td,{children:"Update existing record"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"comparisonresult-interface",children:"ComparisonResult Interface"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ComparisonResult<TEntity>"})," interface wraps the comparison status with optional existing data. The generic type ",(0,s.jsx)(n.code,{children:"TEntity"})," must extend ",(0,s.jsx)(n.code,{children:"DataModel"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { DataModel } from '@mbc-cqrs-serverless/core'\n\nexport interface ComparisonResult<TEntity extends DataModel> {\n  status: ComparisonStatus;  // The result of the comparison\n  /**\n   * If the status is 'CHANGED', this property holds the existing entity data\n   * retrieved from the database. It is undefined otherwise.\n   */\n  existingData?: TEntity;\n}\n"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Property"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"status"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ComparisonStatus"})}),(0,s.jsx)(n.td,{children:"The comparison result status"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"existingData"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"TEntity | undefined"})}),(0,s.jsx)(n.td,{children:"The existing entity data, present when status is CHANGED"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"iprocessstrategy-interface",children:"IProcessStrategy Interface"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"IProcessStrategy"})," interface defines the contract for processing validated import data. Note that the generic type ",(0,s.jsx)(n.code,{children:"TEntity"})," must extend ",(0,s.jsx)(n.code,{children:"DataModel"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import {\n  CommandInputModel,\n  CommandPartialInputModel,\n  CommandService,\n  DataModel,\n} from '@mbc-cqrs-serverless/core';\n\nexport interface IProcessStrategy<TEntity extends DataModel, TAttributesDto extends object> {\n  /**\n   * Compare the validated DTO with existing data\n   */\n  compare(\n    importAttributes: TAttributesDto,\n    tenantCode: string,\n  ): Promise<ComparisonResult<TEntity>>;\n\n  /**\n   * Map the DTO to a command payload based on comparison status\n   * Note: status excludes EQUAL since no mapping is needed for identical data\n   * @returns CommandInputModel for create, CommandPartialInputModel for update\n   */\n  map(\n    status: Exclude<ComparisonStatus, ComparisonStatus.EQUAL>,\n    importAttributes: TAttributesDto,\n    tenantCode: string,\n    existingData?: TEntity,\n  ): Promise<CommandInputModel | CommandPartialInputModel>;\n\n  /**\n   * Get the command service for publishing commands\n   */\n  getCommandService(): CommandService;\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"baseprocessstrategy-class",children:"BaseProcessStrategy Abstract Class"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"BaseProcessStrategy"})," abstract class provides a base implementation that subclasses must extend. The generic type ",(0,s.jsx)(n.code,{children:"TEntity"})," must extend ",(0,s.jsx)(n.code,{children:"DataModel"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { DataModel } from '@mbc-cqrs-serverless/core';\n\nexport abstract class BaseProcessStrategy<TEntity extends DataModel, TTransformedDto extends object>\n  implements IProcessStrategy<TEntity, TTransformedDto>\n{\n  /**\n   * Abstract method - must be implemented to compare data\n   */\n  abstract compare(\n    transformedData: TTransformedDto,\n    tenantCode: string,\n  ): Promise<ComparisonResult<TEntity>>;\n\n  /**\n   * Abstract method - must be implemented to map data to command payload\n   * Note: status excludes EQUAL since no mapping is needed for identical data\n   */\n  abstract map(\n    status: Exclude<ComparisonStatus, ComparisonStatus.EQUAL>,\n    transformedData: TTransformedDto,\n    tenantCode: string,\n    existingData?: TEntity,\n  ): Promise<CommandInputModel | CommandPartialInputModel>;\n\n  /**\n   * Abstract method - must be implemented to return the command service\n   */\n  abstract getCommandService(): CommandService;\n}\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"Note",type:"info",children:(0,s.jsxs)(n.p,{children:["All three methods (",(0,s.jsx)(n.code,{children:"compare()"}),", ",(0,s.jsx)(n.code,{children:"map()"}),", ",(0,s.jsx)(n.code,{children:"getCommandService()"}),") in ",(0,s.jsx)(n.code,{children:"BaseProcessStrategy"})," are abstract and must be implemented by subclasses."]})}),"\n",(0,s.jsx)(n.h3,{id:"implementing-process-strategy",children:"Implementing Process Strategy"}),"\n",(0,s.jsx)(n.p,{children:"The process strategy contains core business logic for comparing and mapping data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { Injectable } from '@nestjs/common';\nimport {\n  CommandInputModel,\n  CommandPartialInputModel,\n  CommandService,\n  DataService,\n} from '@mbc-cqrs-serverless/core';\nimport {\n  BaseProcessStrategy,\n  ComparisonResult,\n  ComparisonStatus,\n  IProcessStrategy,\n} from '@mbc-cqrs-serverless/import';\nimport { PolicyCommandDto } from '../dto/policy-command.dto';\nimport { PolicyDataEntity } from '../entity/policy-data.entity';\n\n@Injectable()\nexport class PolicyProcessStrategy\n  extends BaseProcessStrategy<PolicyDataEntity, PolicyCommandDto>\n  implements IProcessStrategy<PolicyDataEntity, PolicyCommandDto>\n{\n  constructor(\n    private readonly commandService: CommandService,\n    private readonly dataService: DataService,\n  ) {\n    super();\n  }\n\n  getCommandService(): CommandService {\n    return this.commandService;\n  }\n\n  async compare(\n    dto: PolicyCommandDto,\n    tenantCode: string,\n  ): Promise<ComparisonResult<PolicyDataEntity>> {\n    const existing = await this.dataService.getItem({ pk: dto.pk, sk: dto.sk });\n    if (!existing) return { status: ComparisonStatus.NOT_EXIST };\n    return { status: ComparisonStatus.EQUAL, existingData: existing as PolicyDataEntity };\n  }\n\n  async map(\n    status: Exclude<ComparisonStatus, ComparisonStatus.EQUAL>,\n    dto: PolicyCommandDto,\n    tenantCode: string,\n    existingData?: PolicyDataEntity,\n  ): Promise<CommandInputModel | CommandPartialInputModel> {\n    if (status === ComparisonStatus.NOT_EXIST) {\n      // Return CommandInputModel for creating new records\n      return { ...dto, version: 0 } as CommandInputModel;\n    }\n    // status === ComparisonStatus.CHANGED\n    // Return CommandPartialInputModel for updating existing records\n    return {\n      pk: dto.pk,\n      sk: dto.sk,\n      attributes: dto.attributes,\n      version: existingData.version,\n    } as CommandPartialInputModel;\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"module-configuration",children:"Module Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Register the ImportModule with your profiles:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { Module } from '@nestjs/common';\nimport { ImportModule } from '@mbc-cqrs-serverless/import';\nimport { PolicyModule } from './policy/policy.module';\nimport { PolicyImportStrategy } from './policy/strategies/policy.import-strategy';\nimport { PolicyProcessStrategy } from './policy/strategies/policy.process-strategy';\n\n@Module({\n  imports: [\n    PolicyModule,\n    ImportModule.register({\n      enableController: true,\n      imports: [PolicyModule],\n      profiles: [\n        {\n          tableName: 'policy',\n          importStrategy: PolicyImportStrategy,\n          processStrategy: PolicyProcessStrategy,\n        },\n      ],\n    }),\n  ],\n})\nexport class AppModule {}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"custom-event-factory-for-imports",children:"Custom Event Factory for Imports"}),"\n",(0,s.jsx)(n.p,{children:"Configure the event factory to handle import events:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import {\n  EventFactory,\n  IEvent,\n  StepFunctionsEvent,\n} from '@mbc-cqrs-serverless/core';\nimport {\n  CsvImportSfnEvent,\n  DEFAULT_IMPORT_ACTION_QUEUE,\n  ImportEvent,\n  ImportQueueEvent,\n} from '@mbc-cqrs-serverless/import';\nimport { DynamoDBStreamEvent, SQSEvent } from 'aws-lambda';\n\n@EventFactory()\nexport class CustomEventFactory extends EventFactoryAddedTask {\n  async transformDynamodbStream(event: DynamoDBStreamEvent): Promise<IEvent[]> {\n    const curEvents = await super.transformDynamodbStream(event);\n\n    const importEvents = event.Records.map((record) => {\n      if (\n        record.eventSourceARN.endsWith('import_tmp') ||\n        record.eventSourceARN.includes('import_tmp/stream/')\n      ) {\n        if (record.eventName === 'INSERT') {\n          return new ImportEvent().fromDynamoDBRecord(record);\n        }\n      }\n      return undefined;\n    }).filter((event) => !!event);\n\n    return [...curEvents, ...importEvents];\n  }\n\n  async transformSqs(event: SQSEvent): Promise<IEvent[]> {\n    const curEvents = await super.transformSqs(event);\n\n    const importEvents = event.Records.map((record) => {\n      if (record.eventSourceARN.endsWith(DEFAULT_IMPORT_ACTION_QUEUE)) {\n        return new ImportQueueEvent().fromSqsRecord(record);\n      }\n      return undefined;\n    }).filter((event) => !!event);\n\n    return [...importEvents, ...curEvents];\n  }\n\n  async transformStepFunction(event: StepFunctionsEvent<any>): Promise<IEvent[]> {\n    if (event.context.StateMachine.Name.includes('import-csv')) {\n      return [new CsvImportSfnEvent(event)];\n    }\n    return super.transformStepFunction(event);\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"importstatushandler-api",children:"ImportStatusHandler API"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ImportStatusHandler"})," is an internal event handler that manages Step Functions callbacks for import jobs. When using Step Functions orchestration (ZIP imports or STEP_FUNCTION mode CSV imports), this handler ensures proper communication with the state machine."]}),"\n",(0,s.jsx)(n.h4,{id:"behavior",children:"Behavior"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Import Status"}),(0,s.jsx)(n.th,{children:"Action"}),(0,s.jsx)(n.th,{children:"Step Functions Command"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"COMPLETED"})}),(0,s.jsx)(n.td,{children:"Send success callback"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"SendTaskSuccessCommand"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"FAILED"})}),(0,s.jsx)(n.td,{children:"Send failure callback"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"SendTaskFailureCommand"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Other statuses"}),(0,s.jsx)(n.td,{children:"Ignored"}),(0,s.jsx)(n.td,{children:"None"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"sendTaskSuccess(taskToken, output)"})}),(0,s.jsx)(n.td,{children:"Sends success signal to Step Functions with the import result"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"sendTaskFailure(taskToken, error, cause)"})}),(0,s.jsx)(n.td,{children:"Sends failure signal to Step Functions with error details"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"step-functions-integration",children:"Step Functions Integration"}),"\n",(0,s.jsxs)(n.p,{children:["When an import job is created as part of a Step Functions workflow (e.g., ZIP import), a ",(0,s.jsx)(n.code,{children:"taskToken"})," is stored in the job's attributes. The ",(0,s.jsx)(n.code,{children:"ImportStatusHandler"})," listens for status change notifications and:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Retrieves the import job from DynamoDB"}),"\n",(0,s.jsxs)(n.li,{children:["Checks if a ",(0,s.jsx)(n.code,{children:"taskToken"})," exists in the job's attributes"]}),"\n",(0,s.jsxs)(n.li,{children:["Sends the appropriate callback based on the final status:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"COMPLETED"})," \u2192 ",(0,s.jsx)(n.code,{children:"SendTaskSuccessCommand"})," with result data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"FAILED"})," \u2192 ",(0,s.jsx)(n.code,{children:"SendTaskFailureCommand"})," with error details"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This ensures Step Functions workflows properly handle both success and failure cases without hanging indefinitely."}),"\n",(0,s.jsx)(n.admonition,{title:"Version Note",type:"info",children:(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"sendTaskFailure()"})," method was added in ",(0,s.jsx)(n.a,{href:"./changelog#v1018",children:"version 1.0.18"})," to fix an issue where Step Functions would wait indefinitely when import jobs failed. See also ",(0,s.jsx)(n.a,{href:"./error-catalog#import-module-errors",children:"Import Module Errors"})," for troubleshooting."]})}),"\n",(0,s.jsx)(n.h3,{id:"import-error-handling",children:"ImportQueueEventHandler Error Handling"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ImportQueueEventHandler"})," processes individual import records from the SQS queue. When an error occurs during processing (e.g., ",(0,s.jsx)(n.code,{children:"ConditionalCheckFailedException"}),"), the handler properly updates the parent job status."]}),"\n",(0,s.jsx)(n.h4,{id:"error-flow-v1019",children:"Error Flow (v1.0.19+)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Child Job Error Occurs\n         \u2502\n         \u25bc\nMark Child Job as FAILED\n         \u2502\n         \u25bc\nUpdate Parent Job Counters\n  (incrementParentJobCounters)\n         \u2502\n         \u25bc\nCheck if All Children Complete\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502 Yes     \u2502 No\n    \u25bc         \u25bc\nUpdate Master  {{Wait for\n  Job Status}}     more children\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502 Has     \u2502 All\n    \u2502 Failures\u2502 Succeeded\n    \u25bc         \u25bc\nFAILED    COMPLETED\n         \u2502\n         \u25bc\nImportStatusHandler Triggered\n         \u2502\n         \u25bc\nSendTaskFailure/SendTaskSuccess\n"})}),"\n",(0,s.jsx)(n.h4,{id:"key-methods",children:"Key Methods"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"handleImport(event)"})}),(0,s.jsx)(n.td,{children:"Orchestrates single import record processing with error handling"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"executeStrategy(...)"})}),(0,s.jsx)(n.td,{children:"Executes compare, map, and save lifecycle for a strategy"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"error-handling-behavior",children:"Error Handling Behavior"}),"\n",(0,s.jsx)(n.p,{children:"When a child import job fails:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Child job status is set to ",(0,s.jsx)(n.code,{children:"FAILED"})," with error details"]}),"\n",(0,s.jsxs)(n.li,{children:["Parent job counters are atomically updated (",(0,s.jsx)(n.code,{children:"failedRows"})," incremented)"]}),"\n",(0,s.jsxs)(n.li,{children:["When all children complete, master job status is set based on results:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If ",(0,s.jsx)(n.code,{children:"failedRows > 0"})," \u2192 Master status = ",(0,s.jsx)(n.code,{children:"FAILED"})]}),"\n",(0,s.jsxs)(n.li,{children:["If all succeeded \u2192 Master status = ",(0,s.jsx)(n.code,{children:"COMPLETED"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Lambda does NOT crash - error is handled gracefully"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Common Errors",type:"warning",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"ConditionalCheckFailedException"}),": This occurs when attempting to import data that already exists with conflicting version. The import job will be marked as FAILED and the parent job will properly aggregate this failure."]})}),"\n",(0,s.jsx)(n.admonition,{title:"Version Note",type:"info",children:(0,s.jsxs)(n.p,{children:["Prior to v1.0.19, errors in child jobs would crash the Lambda and leave the master job in ",(0,s.jsx)(n.code,{children:"PROCESSING"})," status indefinitely. The fixes in ",(0,s.jsx)(n.a,{href:"./changelog#v1019",children:"version 1.0.19"})," ensure proper error propagation and status updates."]})}),"\n",(0,s.jsx)(n.h3,{id:"csvimportsfneventhandler",children:"CsvImportSfnEventHandler"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"CsvImportSfnEventHandler"})," handles Step Functions CSV import workflow states. It manages the ",(0,s.jsx)(n.code,{children:"csv_loader"})," and ",(0,s.jsx)(n.code,{children:"finalize_parent_job"})," states in the import state machine."]}),"\n",(0,s.jsx)(n.h4,{id:"key-methods-1",children:"Key Methods"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"handleStepState(event)"})}),(0,s.jsxs)(n.td,{children:["Routes events to appropriate handlers based on state name (",(0,s.jsx)(n.code,{children:"csv_loader"})," or ",(0,s.jsx)(n.code,{children:"finalize_parent_job"}),")"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"loadCsv(input)"})}),(0,s.jsx)(n.td,{children:"Processes the csv_loader state, creates child jobs for CSV rows"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"finalizeParentJob(event)"})}),(0,s.jsx)(n.td,{children:"Finalizes the parent job after all children complete, sets final status"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"status-determination-v1020",children:"Status Determination (v1.0.20+)"}),"\n",(0,s.jsx)(n.p,{children:"When finalizing the parent job, the handler correctly determines the final status:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Correct behavior (v1.0.20+)\nconst status = failedRows > 0\n  ? ImportJobStatus.FAILED    // Any child failed \u2192 FAILED\n  : ImportJobStatus.COMPLETED // All children succeeded \u2192 COMPLETED\n"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Known Issue (Fixed in v1.0.20)",type:"warning",children:[(0,s.jsxs)(n.p,{children:["Prior to v1.0.20, a bug in the ternary operator caused the status to always be ",(0,s.jsx)(n.code,{children:"COMPLETED"}),":"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Bug (pre-v1.0.20): always returned COMPLETED\nconst status = failedRows > 0\n  ? ImportJobStatus.COMPLETED  // Wrong!\n  : ImportJobStatus.COMPLETED\n"})}),(0,s.jsxs)(n.p,{children:["This caused Step Functions to report SUCCESS even when child import jobs failed. See ",(0,s.jsx)(n.a,{href:"./changelog#v1020",children:"version 1.0.20"})," for details."]})]}),"\n",(0,s.jsx)(n.h3,{id:"zipimportsfneventhandler",children:"ZipImportSfnEventHandler"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ZipImportSfnEventHandler"})," handles Step Functions ZIP import workflow states. It orchestrates the processing of multiple CSV files extracted from a ZIP archive."]}),"\n",(0,s.jsx)(n.h4,{id:"workflow-states",children:"Workflow States"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"State"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"trigger_single_csv_and_wait"})}),(0,s.jsx)(n.td,{children:"Triggers a single CSV import job for each file in the ZIP"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"finalize_zip_job"})}),(0,s.jsx)(n.td,{children:"Aggregates results from all CSV imports and finalizes the master job"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"key-methods-2",children:"Key Methods"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"triggerSingleCsvJob(event)"})}),(0,s.jsx)(n.td,{children:"Creates a CSV import job with STEP_FUNCTION mode, passing the taskToken for callback"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"finalizeZipMasterJob(event)"})}),(0,s.jsx)(n.td,{children:"Aggregates results from all processed CSV files and updates the ZIP master job status"})]})]})]}),"\n",(0,s.jsxs)(n.admonition,{title:"Known Issue",type:"warning",children:[(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"finalizeZipMasterJob"})," method currently always sets the master job status to ",(0,s.jsx)(n.code,{children:"COMPLETED"}),", regardless of whether any child CSV import jobs failed. This means ZIP import workflows will report success even when individual CSV files failed to import correctly."]}),(0,s.jsxs)(n.p,{children:["To work around this, check the ",(0,s.jsx)(n.code,{children:"failedRows"})," count in the result object to determine if any errors occurred during processing."]})]}),"\n",(0,s.jsx)(n.h4,{id:"file-naming-convention",children:"File Naming Convention"}),"\n",(0,s.jsx)(n.p,{children:"When processing CSV files from a ZIP archive, the handler extracts the table name from the filename:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Format: yyyymmddhhMMss-\\{tableName\\}.csv\nExample: 20240115120000-products.csv \u2192 extracts tableName = "products"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If ",(0,s.jsx)(n.code,{children:"tableName"})," is provided in the ",(0,s.jsx)(n.code,{children:"CreateZipImportDto"}),", it overrides the extracted name."]}),"\n",(0,s.jsx)(n.h4,{id:"processing-flow",children:"Processing Flow"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"ZIP File Uploaded to S3\n         \u2502\n         \u25bc\nStep Functions Triggered\n         \u2502\n         \u25bc\nUnzip and List CSV Files\n         \u2502\n         \u25bc\nMap State: For Each CSV File\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n    \u25bc         \u25bc\ntrigger_single_csv_and_wait\n    \u2502         \u2502\n    \u25bc         \u25bc\nCSV Import Job Created    CSV Import Job Created\n(with taskToken)          (with taskToken)\n    \u2502         \u2502\n    \u25bc         \u25bc\nWait for Completion       Wait for Completion\n    \u2502         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\nfinalize_zip_job\n         \u2502\n         \u25bc\nAggregate Results & Update Master Job\n"})}),"\n",(0,s.jsx)(n.h4,{id:"zipimportsfnevent-structure",children:"ZipImportSfnEvent Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"export class ZipImportSfnEvent implements IEvent {\n  source: string           // Execution ID from Step Functions\n  context: StepFunctionsContext // Step Functions context with state info\n  input: string | any[]    // S3 key or array of results\n  taskToken: string        // Token for callback to Step Functions\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"context.Execution.Input"})," contains:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"masterJobKey"}),": Primary key of the ZIP master job in DynamoDB"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"parameters"}),": Original import parameters (bucket, tenantCode, tableName)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"./backend-development",children:"Backend Development Guide"})," - Core backend patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"./service-patterns",children:"Service Patterns"})," - Service implementation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"./architecture/step-functions",children:"Step Functions"})," - Workflow orchestration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"./data-sync-handler-examples",children:"Data Sync Handler Examples"})," - Sync handler patterns"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var r=t(6540);const s={},o=r.createContext(s);function i(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);